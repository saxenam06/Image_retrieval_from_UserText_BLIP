{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saxenam06/Image_search_UserText/blob/main/Store_Embeddings_LargeImageDrivingDataset_UsingBLIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBmTibAh25hK"
      },
      "source": [
        "# Mount Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09tKDls6IGnL",
        "outputId": "e23dba89-1ec0-4e13-92cb-2edfd8d335b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_KsNcsy3Lkm"
      },
      "source": [
        "# Install Requirements, Load Libraries. Download & Define Dataset path, Create Spark Session\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2BoodUM3bIO"
      },
      "source": [
        "### Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USDyQ_OZdQD_",
        "outputId": "671b1505-9938-4640-de37-8f85dcbcc92b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=7c94c1732b346d61e68bf9f36860a2e4b3249bbfcca34c98b54e83d9a5014020\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n",
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.1-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-autotime) (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->ipython-autotime)\n",
            "  Downloading jedi-0.19.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.6)\n",
            "Installing collected packages: jedi, ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.1 jedi-0.19.0\n",
            "time: 1.04 ms (started: 2023-09-07 10:35:42 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install pyspark\n",
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQwd_SuK3iDd"
      },
      "source": [
        "### Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFp2O7RAm974",
        "outputId": "4120dd0e-d25f-415a-d2e0-6696c2c54f30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.03 s (started: 2023-09-07 10:35:42 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os.path as osp\n",
        "import os\n",
        "import sys\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import base64\n",
        "from io import BytesIO\n",
        "import io\n",
        "import multiprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, pandas_udf, explode, PandasUDFType\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, BinaryType, DoubleType, FloatType"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download & Unzip data, Define DataSet Path\n",
        "Download, Unzip data from https://once-for-auto-driving.github.io/index.html and Place it in your drive. Define Path below for Images and Annotations(MetaData) as per the drive location."
      ],
      "metadata": {
        "id": "jLhj3RcV6Yvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/drive/MyDrive\"\n",
        "train_split_igs_folder = \"ONCE_Training_split_cam_igs_unzip\"\n",
        "train_split_annotns_folder = \"ONCE_Training_split_annotations_unzip\"\n",
        "train_split_igs_path = osp.join(data_path, train_split_igs_folder, 'data', '000076') # ,\n",
        "train_split_annotns_path =osp.join(data_path, train_split_annotns_folder, 'data', '000076') # , '000076', , '000495'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqBKTJVn6U3Y",
        "outputId": "1cd3c7b5-48a3-45b1-e45b-44d649a40a21"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4.27 ms (started: 2023-09-07 10:35:43 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0p2L6N63gFl"
      },
      "source": [
        "### Create Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "0W6y2QYvc8t5",
        "outputId": "e9d0c471-7ec1-477a-fdc3-832246da236b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7d65854ebc70>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://7c0ab9d567d3:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Images_Embed</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 13.3 s (started: 2023-09-07 10:35:43 +00:00)\n"
          ]
        }
      ],
      "source": [
        "spark = SparkSession.builder.appName(\"Images_Embed\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimize configuration (As of now default!)"
      ],
      "metadata": {
        "id": "BmVGmrlDOpXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext\n",
        "sc.getConf().getAll()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWjQc0KwO9FF",
        "outputId": "dfda3205-450a-4861-ad9f-2709eb2ad7a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spark.app.name', 'Images_Embed'),\n",
              " ('spark.driver.port', '34319'),\n",
              " ('spark.driver.host', '7c0ab9d567d3'),\n",
              " ('spark.executor.id', 'driver'),\n",
              " ('spark.sql.warehouse.dir', 'file:/content/spark-warehouse'),\n",
              " ('spark.app.id', 'local-1694082950827'),\n",
              " ('spark.driver.extraJavaOptions',\n",
              "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
              " ('spark.app.submitTime', '1694082947641'),\n",
              " ('spark.rdd.compress', 'True'),\n",
              " ('spark.serializer.objectStreamReset', '100'),\n",
              " ('spark.master', 'local[*]'),\n",
              " ('spark.submit.pyFiles', ''),\n",
              " ('spark.submit.deployMode', 'client'),\n",
              " ('spark.ui.showConsoleProgress', 'true'),\n",
              " ('spark.app.startTime', '1694082947944'),\n",
              " ('spark.executor.extraJavaOptions',\n",
              "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 121 ms (started: 2023-09-07 10:35:56 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4suXwuvdNI0"
      },
      "source": [
        "# Ingest Available images, Preprocess DataFrame, Load the Metadata, Integrate image dataframe with the Metadata\n",
        "\n",
        "### Around 95k images and around 1000k JSON files consisiting Metadata and annotations for every image. Also find annotations from the JSON files for the available images and integrate to the image dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZErOLio4Nel"
      },
      "source": [
        "### Load Images in binary format from drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RmDgAcfdV2T",
        "outputId": "25bd772b-23c2-4b10-9fcf-4ff6d71cd3b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 41.5 s (started: 2023-09-07 10:35:56 +00:00)\n"
          ]
        }
      ],
      "source": [
        "sample_img_dir = train_split_igs_path\n",
        "spark = SparkSession.builder.appName(\"Images_Embed\").getOrCreate()\n",
        "image_df = spark.read.format(\"binaryFile\") \\\n",
        "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
        "  .option(\"recursiveFileLookup\", \"true\") \\\n",
        "  .load(sample_img_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_df.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_jytugDQPLM",
        "outputId": "446a5023-9eaa-4c3c-a3e5-f6f944a1ecf0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.56 s (started: 2023-09-07 10:36:38 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrKCUdbyi1S_"
      },
      "source": [
        "### Preprocess: Add column for Frame_id (Optionally also for Seq_id and cam_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOWMXdFjs9t5",
        "outputId": "80814bee-08e9-49a9-9e2b-33431e02fd1f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- path: string (nullable = true)\n",
            " |-- modificationTime: timestamp (nullable = true)\n",
            " |-- length: long (nullable = true)\n",
            " |-- content: binary (nullable = true)\n",
            "\n",
            "time: 16.3 ms (started: 2023-09-07 10:36:39 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlRoDNC4gaWp",
        "outputId": "8131251b-e26c-487f-ca61-e307fd1e482c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- path: string (nullable = true)\n",
            " |-- modificationTime: timestamp (nullable = true)\n",
            " |-- length: long (nullable = true)\n",
            " |-- content: binary (nullable = true)\n",
            " |-- Frame_id: string (nullable = true)\n",
            "\n",
            "time: 107 ms (started: 2023-09-07 10:36:39 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def extract_cam_id(filepath):\n",
        "    return os.path.basename(os.path.dirname(filepath))\n",
        "\n",
        "def extract_seq_id(filepath):\n",
        "    path_components = filepath.split('/')\n",
        "    return path_components[-3]\n",
        "\n",
        "def extract_frame_id(filepath):\n",
        "    path_components = filepath.split('/')\n",
        "    path_components = path_components[-1].split('.')\n",
        "    return path_components[0]\n",
        "\n",
        "# Register UDFs\n",
        "\n",
        "extract_cam_id_udf = udf(extract_cam_id, StringType())\n",
        "extract_seq_id_udf = udf(extract_seq_id, StringType())\n",
        "extract_frame_id_udf = udf(extract_frame_id, StringType())\n",
        "image_df = image_df.withColumn(\"Frame_id\", extract_frame_id_udf(image_df[\"path\"]))\n",
        "image_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_df = image_df.drop('length', 'modificationTime', 'path')\n",
        "image_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xd_JRsRStGFL",
        "outputId": "703232bd-ba50-453d-b6d5-440e2163c3f9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- content: binary (nullable = true)\n",
            " |-- Frame_id: string (nullable = true)\n",
            "\n",
            "time: 34.8 ms (started: 2023-09-07 10:36:40 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWhsmRiv5zdC"
      },
      "source": [
        "### Define Schema and Load Metadata in JSON files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AbuSS_S96Ub",
        "outputId": "50c47d80-318f-41dc-b5b0-1550aa0f6d69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.9 s (started: 2023-09-07 10:36:40 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Define the schema\n",
        "schema = StructType([\n",
        "    StructField(\"meta_info\", StructType([\n",
        "        StructField(\"weather\", StringType(), nullable=True),\n",
        "        StructField(\"period\", StringType(), nullable=True),\n",
        "    ])),\n",
        "    StructField(\"frames\", ArrayType(StructType([\n",
        "        StructField(\"frame_id\", StringType(), nullable=True),\n",
        "    ])), nullable=True)\n",
        "])\n",
        "\n",
        "# Load JSON files and create a DataFrame\n",
        "root_folder = train_split_annotns_path\n",
        "\n",
        "json_df = spark.read.option(\"multiline\", \"true\").json(root_folder + \"/*\", schema=schema)\n",
        "\n",
        "# Explode the frames array to individual rows\n",
        "json_df = json_df.select(\n",
        "    col(\"meta_info.weather\").alias(\"weather\"),\n",
        "    col(\"meta_info.period\").alias(\"period\"),\n",
        "    explode(col(\"frames\")).alias(\"frames\"),\n",
        ").selectExpr(\n",
        "    \"weather\", \"period\",\n",
        "    \"frames.frame_id AS Frame_id\",)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSqfVivI6Mpy"
      },
      "source": [
        "### Check metadata and image schemas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEAZudKk-QM6",
        "outputId": "37813968-76a9-4c9b-86e3-ded584dc1323"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- weather: string (nullable = true)\n",
            " |-- period: string (nullable = true)\n",
            " |-- Frame_id: string (nullable = true)\n",
            "\n",
            "time: 9.17 ms (started: 2023-09-07 10:36:45 +00:00)\n"
          ]
        }
      ],
      "source": [
        "json_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "So1Lqf8JtxVB",
        "outputId": "49ff321b-34da-40d8-88c5-6c37da56d278"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- content: binary (nullable = true)\n",
            " |-- Frame_id: string (nullable = true)\n",
            "\n",
            "time: 10.2 ms (started: 2023-09-07 10:36:46 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obv9BFbTOI9q"
      },
      "source": [
        "### Integrate Metadata with Image dataframe.\n",
        "Add columns corresponding to weather, period and Frame_id from the JSON files to image_df using Frame_id as the joining key"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_cols = [\"Frame_id\", \"weather\", \"period\"]\n",
        "selected_json_df = json_df.select(*selected_cols)\n",
        "image_df = image_df.join(selected_json_df, on=\"Frame_id\", how=\"inner\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVQREs32Vt7U",
        "outputId": "be44efdf-1a1f-45de-e4cb-b520044dc0a5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 148 ms (started: 2023-09-07 10:36:46 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0E-ktTKOcfT"
      },
      "source": [
        "# Load reqd. Libraries for BLIP, Download BLIP model, Distribute model to every Spark executor node, Wrap model in Pandas UDF and Predict Embeddings using BLIP\n",
        "Multi-node distributed inference of the model with PyTorch and Pandas UDF.\n",
        "We use Pandas user-defined functions on Spark to implement a distributed, parallel approach for model inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAGZkZgEOjH6"
      },
      "source": [
        "### Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tuj_4o7OcRR",
        "outputId": "8a6c888d-4fd8-4bcb-81bd-f81357ec688e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fairscale\n",
            "  Downloading fairscale-0.4.13.tar.gz (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Collecting huggingface-hub (from timm)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors (from timm)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from fairscale) (1.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Building wheels for collected packages: fairscale\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332106 sha256=90416e1d5d22f13c0aba55a832023dd495ffeec645e71a48771edeab23ed111c\n",
            "  Stored in directory: /root/.cache/pip/wheels/78/a4/c0/fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3\n",
            "Successfully built fairscale\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers, timm, fairscale\n",
            "Successfully installed fairscale-0.4.13 huggingface-hub-0.16.4 safetensors-0.3.3 timm-0.9.7 tokenizers-0.13.3 transformers-4.33.1\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.14.5 dill-0.3.7 multiprocess-0.70.15 xxhash-3.3.0\n",
            "time: 49.1 s (started: 2023-09-07 10:36:46 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!pip install timm fairscale transformers\n",
        "!pip install datasets torch\n",
        "# install requirements\n",
        "from pyspark import SparkFiles\n",
        "import zipfile\n",
        "import torch\n",
        "import shutil\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download BLIP repo"
      ],
      "metadata": {
        "id": "gQepxXUXZa9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'google.colab' in sys.modules:\n",
        "    print('Running in Colab.')\n",
        "    !git clone https://github.com/salesforce/BLIP\n",
        "    %cd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YIR5IVLwzeC",
        "outputId": "5f4da4d1-86ab-4362-cff6-bb0ad1a23b93"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Colab.\n",
            "Cloning into 'BLIP'...\n",
            "remote: Enumerating objects: 277, done.\u001b[K\n",
            "remote: Counting objects: 100% (165/165), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 277 (delta 137), reused 136 (delta 135), pack-reused 112\u001b[K\n",
            "Receiving objects: 100% (277/277), 7.03 MiB | 14.88 MiB/s, done.\n",
            "Resolving deltas: 100% (153/153), done.\n",
            "/root\n",
            "time: 2.32 s (started: 2023-09-07 10:37:35 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distribute repo to every Spark executor node"
      ],
      "metadata": {
        "id": "egDlH4cOZrZ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29w09sHYVf6M",
        "outputId": "ad7f107a-4bba-4555-8c08-d28823a62a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 112 ms (started: 2023-09-07 10:37:37 +00:00)\n"
          ]
        }
      ],
      "source": [
        "sc = spark.sparkContext\n",
        "\n",
        "# Distribute the ZIP archives\n",
        "shutil.make_archive(\"/content/BLIP/models_zip\", 'zip', \"/content/BLIP\", \"models\")\n",
        "shutil.make_archive(\"/content/BLIP/configs_zip\", 'zip', \"/content/BLIP\", \"configs\")\n",
        "\n",
        "# Add the ZIP files to Spark\n",
        "models_zip_path = '/content/BLIP/models_zip.zip'\n",
        "configs_zip_path = '/content/BLIP/configs_zip.zip'\n",
        "\n",
        "sc = spark.sparkContext\n",
        "sc.addPyFile(models_zip_path)\n",
        "sc.addPyFile(configs_zip_path)\n",
        "\n",
        "sys.path.append('/content/BLIP/models')\n",
        "sys.path.append('/content/BLIP/configs')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTIkcbuoBQEZ"
      },
      "source": [
        "### Wrap model in Pandas UDF and Do distributed inference for Creating Embeddings using BLIP- model_base_retrieval_coco\n",
        "\n",
        "Let Spark optimize the execution instead of writing imperative RDD code.\n",
        "\n",
        "Define a Pandas UDF for the inference task and wrap the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlhXC75PSo3o",
        "outputId": "48df8a57-569d-4576-e738-5d110a447e7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 6.7 s (started: 2023-09-07 10:37:37 +00:00)\n"
          ]
        }
      ],
      "source": [
        "from models.blip_itm import blip_itm\n",
        "\n",
        "def initialize_blip_itm_model():\n",
        "    model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth'\n",
        "    model = blip_itm(pretrained=model_url, image_size=384, vit='base')\n",
        "    model.eval()\n",
        "    model = model.to(device='cpu')\n",
        "    return model\n",
        "\n",
        "# Define the schema for the output DataFrame\n",
        "schema = StructType([\n",
        "    StructField('Embeddings', ArrayType(FloatType()))\n",
        "])\n",
        "\n",
        "# Define a Pandas UDF to calculate ITM and ITC scores\n",
        "@pandas_udf(schema)\n",
        "def calculate_embeds(content_series: pd.Series) -> pd.DataFrame:\n",
        "\n",
        "    models_zip_path_on_workers = SparkFiles.get('models_zip.zip')\n",
        "    configs_zip_path_on_workers = SparkFiles.get('configs_zip.zip')\n",
        "\n",
        "    models_subdirectory_path = '/content/'\n",
        "    configs_subdirectory_path = '/content/'\n",
        "\n",
        "    print(f\"Extracting models to {models_subdirectory_path}\")\n",
        "    with zipfile.ZipFile(models_zip_path_on_workers, 'r') as zip_ref:\n",
        "      zip_ref.extractall(models_subdirectory_path)\n",
        "\n",
        "    print(f\"Extracting models to {configs_subdirectory_path}\")\n",
        "    with zipfile.ZipFile(configs_zip_path_on_workers, 'r') as zip_ref:\n",
        "      zip_ref.extractall(configs_subdirectory_path)\n",
        "\n",
        "    med_config_path = '/content/configs/med_config.json'\n",
        "    with open(med_config_path, 'r', encoding='utf-8') as reader:\n",
        "      med_config_content = reader.read()\n",
        "    print(f\"med_config_path: {med_config_path}\")\n",
        "\n",
        "    model = initialize_blip_itm_model()\n",
        "    model.eval()\n",
        "    embeddings_list = []\n",
        "\n",
        "    for content in content_series:\n",
        "        image = Image.open(io.BytesIO(content))\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((384, 384), interpolation=InterpolationMode.BICUBIC),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "        ])\n",
        "        image_tensor = transform(image).unsqueeze(0).to(device='cpu')  # Convert image to tensor\n",
        "\n",
        "        # Perform your BLIP model calculations here\n",
        "        image_embeds = model.visual_encoder(image_tensor)\n",
        "        # embeddings_list.append(image_embeds.squeeze().detach().cpu().numpy())  # Append the tensor as isW\n",
        "\n",
        "        # Flatten the image_embeds tensor and append it to the list\n",
        "        flattened_embeds = image_embeds.squeeze().view(-1).detach().cpu().numpy()\n",
        "        embeddings_list.append(flattened_embeds)\n",
        "\n",
        "    return pd.DataFrame({'Embeddings': embeddings_list})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_df_embeds = image_df.withColumn(\"Embeddings\", calculate_embeds(col(\"content\")))\n",
        "image_df_embeds.show(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShwJJn-oyNuf",
        "outputId": "b5a50942-9acf-4630-d89b-19d61bc17866"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------------------+-------+-------+--------------------+\n",
            "|     Frame_id|             content|weather| period|          Embeddings|\n",
            "+-------------+--------------------+-------+-------+--------------------+\n",
            "|1616343577200|[FF D8 FF E0 00 1...|  sunny|morning|{[-0.39624017, 0....|\n",
            "+-------------+--------------------+-------+-------+--------------------+\n",
            "only showing top 1 row\n",
            "\n",
            "time: 3min 3s (started: 2023-09-07 10:37:44 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cache the Spark DataFrame and Write the Embeddings in Parquet format"
      ],
      "metadata": {
        "id": "ix6svP0o8oaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select and write the desired columns\n",
        "image_df_embeds.cache()\n",
        "image_df_embeds.select(\"Frame_id\", \"Embeddings\") \\\n",
        "        .write.mode(\"overwrite\") \\\n",
        "        .parquet(\"Frame_Embeds\")"
      ],
      "metadata": {
        "id": "oAHf-vQRycOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Store the Embeddings to desired folder"
      ],
      "metadata": {
        "id": "9yfFdZKL-oWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_path = '/content/Frame_Embeds'\n",
        "destination_directory = '/content/drive/MyDrive/Frame_Embeds_000076'\n",
        "shutil.rmtree(destination_directory, ignore_errors=True)\n",
        "shutil.copytree(result_path, destination_directory)\n"
      ],
      "metadata": {
        "id": "2UHfB8lQK2gn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "TZErOLio4Nel",
        "nrKCUdbyi1S_",
        "nWhsmRiv5zdC",
        "MSqfVivI6Mpy"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}